{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Pipeline Verification (Keerthi)\n",
                "\n",
                "This notebook verifies the implementation of the model loading, LoRA configuration, and training loop logic. \n",
                "It uses a small model and dummy data to ensure the code runs without errors on the local machine."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "\n",
                "# Add src to path\n",
                "sys.path.append(os.path.abspath(os.path.join('..')))\n",
                "\n",
                "from src.model import get_model_and_tokenizer, get_lora_config, get_peft_model_wrapper\n",
                "from src.evaluate_metrics import compute_metrics, evaluate_model\n",
                "import torch"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Test Model Loading (CPU/Mac Friendly)\n",
                "We use `gpt2` as a tiny placeholder to test the logic without downloading 7GB weights."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Disable quantization for local Mac testing\n",
                "model_name = \"gpt2\" \n",
                "model, tokenizer = get_model_and_tokenizer(model_name, use_quantization=False)\n",
                "\n",
                "print(\"Model loaded successfully:\", type(model))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Test LoRA Configuration\n",
                "Apply the LoRA adapter to the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Note: GPT2 uses 'c_attn' instead of 'q_proj', 'v_proj' etc.\n",
                "# For this test, we might need to adjust target_modules temporarily or just check if it initializes.\n",
                "# In the real script, we use Mistral/Llama target modules.\n",
                "\n",
                "try:\n",
                "    peft_config = get_lora_config(r=8, alpha=16)\n",
                "    # Override target modules for GPT2 test\n",
                "    peft_config.target_modules = [\"c_attn\"]\n",
                "    \n",
                "    lora_model = get_peft_model_wrapper(model, peft_config)\n",
                "    print(\"LoRA adapter applied successfully.\")\n",
                "except Exception as e:\n",
                "    print(f\"LoRA Error (expected if modules don't match): {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Test Evaluation Metrics\n",
                "Run a dummy evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dummy_dataset = [\n",
                "    {\"prompt\": \"History of Rome:\", \"completion\": \"Rome was founded in 753 BC.\"},\n",
                "    {\"prompt\": \"Who was Napoleon?\", \"completion\": \"He was a French emperor.\"}\n",
                "]\n",
                "\n",
                "# Ensure pad token is set\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "metrics = evaluate_model(model, tokenizer, dummy_dataset, max_new_tokens=10)\n",
                "print(\"Evaluation Metrics:\", metrics)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}