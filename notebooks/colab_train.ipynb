{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Historical Event Narrator - Training Notebook (Colab)\n",
                "\n",
                "This notebook is designed to run on Google Colab (Free Tier T4 GPU).\n",
                "It clones the repository, installs dependencies, and runs the LoRA fine-tuning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Clone Repository\n",
                "!git clone https://github.com/Keerthirajan58/Historical-Event-Narrator.git\n",
                "%cd Historical-Event-Narrator\n",
                "!git checkout keerthi-dev"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Install Dependencies\n",
                "!pip install -r requirements.txt\n",
                "!pip install bitsandbytes accelerate peft transformers datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Run Training\n",
                "# We use the synthetic data we generated (ensure it's in the repo or upload it)\n",
                "# If data is not in repo (because it's large), you might need to run preprocess here or upload it.\n",
                "# Since we pushed the 50-example sample, we can run a test train.\n",
                "\n",
                "from src.train import train\n",
                "\n",
                "# Run training with Mistral-7B (requires GPU)\n",
                "# Note: On Colab T4, we use 4-bit quantization\n",
                "train(\n",
                "    train_file=\"data/processed/train.jsonl\",\n",
                "    val_file=\"data/processed/test.jsonl\",\n",
                "    model_name=\"mistralai/Mistral-7B-v0.1\",\n",
                "    num_epochs=1,\n",
                "    batch_size=4,\n",
                "    use_quantization=True\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Inference Test\n",
                "from src.model import get_model_and_tokenizer, get_peft_model_wrapper\n",
                "from peft import PeftModel\n",
                "import torch\n",
                "\n",
                "# Load Base Model\n",
                "base_model, tokenizer = get_model_and_tokenizer(\"mistralai/Mistral-7B-v0.1\", use_quantization=True)\n",
                "\n",
                "# Load Adapter\n",
                "model = PeftModel.from_pretrained(base_model, \"./results\")\n",
                "\n",
                "prompt = \"Narrate the historical event with a creative twist.\\n\\nTopic: The Moon Landing\\n\\nNarrative:\"\n",
                "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
                "\n",
                "outputs = model.generate(**inputs, max_new_tokens=200)\n",
                "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}